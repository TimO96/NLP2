{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probing Language Models__\n",
    "\n",
    "This notebook serves as a start for your NLP2 assignment on probing Language Models. This notebook will become part of the contents that you will submit at the end, so make sure to keep your code (somewhat) clean :-)\n",
    "\n",
    "__note__: This is the first time _anyone_ is doing this assignment. That's exciting! But it might well be the case that certain aspects are too unclear. Do not hesitate at all to reach to me once you get stuck, I'd be grateful to help you out.\n",
    "\n",
    "__note 2__: This notebook only contains the setup for the POS probing experiment. The structural probing setup will follow soon, but I'm still working out some things there. This should keep you occupied for now hopefully, good luck!\n",
    "\n",
    "__note 3__: This assignment is not dependent on big fancy GPUs. I run all this stuff on my own 3 year old CPU, without any Colab hassle. So it's up to you to decide how you want to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For the Transformer models you are advised to make use of the `transformers` library of Huggingface: https://github.com/huggingface/transformers\n",
    "Their library is well documented, and they provide great tools to easily load in pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code for initializing the transformer model(s)\n",
    "#\n",
    "# Note that most transformer models use their own `tokenizer`, that should be loaded in as well.\n",
    "#\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2', output_hidden_states = True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', output_hidden_states = True)\n",
    "\n",
    "\n",
    "# Note that some models don't return the hidden states by default.\n",
    "# This can be configured using a `PretrainedConfig` object, or directly like:\n",
    "# model.transformer.output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code for initializing the rnn model(s)\n",
    "#\n",
    "# The Gulordava LSTM model can be found here: \n",
    "# https://github.com/facebookresearch/colorlessgreenRNNs/tree/master/data\n",
    "#\n",
    "from collections import defaultdict\n",
    "from model import RNNModel\n",
    "import torch\n",
    "\n",
    "\n",
    "model_location = 'Gulordava.pt'  # <- point this to the location of the Gulordava .pt file\n",
    "device = 'cpu'\n",
    "rnn = torch.load(model_location, map_location=device)\n",
    "\n",
    "\n",
    "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
    "with open('vocab.txt') as f:\n",
    "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
    "\n",
    "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
    "vocab.update(w2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For this assignment you will train your probes on __treebank__ corpora. A treebank is a corpus that has been *parsed*, and stored in a representation that allows the parse tree to be recovered. Next to a parse tree, treebanks also often contain information about part-of-speech tags, which is exactly what we are after now.\n",
    "\n",
    "The treebank you will use for now is part of the Universal Dependencies project. I provide a sample of this treebank as well, so you can test your setup on that before moving on to larger amounts of data.\n",
    "\n",
    "Make sure you accustom yourself to the format that is created by the `conllu` library that parses the treebank files before moving on. For example, make sure you understand how you can access the pos tag of a token, or how to cope with the tree structure that is formed using the `to_tree()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "from typing import List\n",
    "from conllu import parse_incr, TokenList\n",
    "\n",
    "\n",
    "# If stuff like `: str` and `-> ..` seems scary, fear not! \n",
    "# These are type hints that help you to understand what kind of argument and output is expected.\n",
    "def parse_corpus(filename: str) -> List[TokenList]:\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses\n",
    "\n",
    "tokens = ['PPN', 'NP', 'SUBJ']\n",
    "tokens = {tokens[i]:i for i in range(len(tokens))}\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Representations\n",
    "\n",
    "We now have our data all set, our models are running and we are good to go!\n",
    "\n",
    "The next step is now to create the model representations for the sentences in our corpora. Once we have generated these representations we can store them, and train additional diagnostic (/probing) classifiers on top of the representations.\n",
    "\n",
    "There are a few things you should keep in mind here. Read these carefully, as these tips will save you a lot of time in your implementation.\n",
    "- Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of next in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\".\n",
    "\n",
    "- Subword chunks never overlap multiple tokens. In other words, say we have a phrase like \"None of the\", then the tokenizer might chunk that into \"No\"+\"ne\"+\" of\"+\" the\", but __not__ into \"No\"+\"ne o\"+\"f the\", as those chunks overlap multiple tokens. This is great for our setup! Otherwise it would have been quite challenging to distribute the representation of a subword over the 2 tokens it belongs to.\n",
    "\n",
    "- If you closely examine the provided treebank, you will notice that some tokens are split up into multiple pieces, that each have their own POS-tag. For example, in the first sentence the word \"Al-Zaman\" is split into \"Al\", \"-\", and \"Zaman\". In such cases, the conllu `TokenList` format will add the following attribute: `('misc', OrderedDict([('SpaceAfter', 'No')]))` to these tokens. Your model's tokenizer does not need to adhere to the same tokenization. E.g., \"Al-Zaman\" could be split into \"Al-\"+\"Za\"+\"man\", making it hard to match the representations with their correct pos-tag. Therefore I recommend you to not tokenize your entire sentence at once, but to do this based on the chunking of the treebank. Make sure to still incoporate the spaces in a sentence though, as these are part of the BPE of the tokenizer. The tokenizer for GPT-2 adds spaces at the start of a token.\n",
    "\n",
    "- The LSTM LM does not have the issues related to subwords, but is far more restricted in its vocabulary. Make sure you keep the above points in mind though, when creating the LSTM representations. You might want to write separate functions for the LSTM, but that is up to you.\n",
    "\n",
    "I would like to stress that if you feel hindered in any way by the simple code structure that is presented here, you are free to modify it :-) Just make sure it is clear to an outsider what you're doing, some helpful comments never hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH SENTENCE REPRESENTATIONS\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus, representation_size)\n",
    "# Make sure you correctly average the subword representations that belong to 1 token!\n",
    "def fetch_sen_reps(ud_parses: List[TokenList], model, tokenizer) -> Tensor:\n",
    "    sen_reps = []\n",
    "    for sentence in tqdm(ud_parses):\n",
    "        for word in sentence:\n",
    "            input_ids = Tensor(tokenizer.encode(word['lemma'])).type(torch.long).unsqueeze(0)\n",
    "            output = model(input_ids)[0][0].mean(dim=0).detach()\n",
    "            sen_reps.append(output)\n",
    "    \n",
    "    return torch.stack(sen_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH POS LABELS\n",
    "\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
    "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
    "def fetch_pos_tags(ud_parses: List[TokenList], vocab=None) -> Tensor:\n",
    "    pos_tags = []\n",
    "    vocab_list = []\n",
    "    for sentence in ud_parses:\n",
    "        for word in sentence:\n",
    "            pos_tag = word['upostag']\n",
    "            \n",
    "            if vocab:\n",
    "                if pos_tag not in vocab:\n",
    "                    pos_tags.append(vocab['unk'])\n",
    "                \n",
    "                else:\n",
    "                    pos_tags.append(vocab[pos_tag])\n",
    "            \n",
    "            else:\n",
    "                if pos_tag not in vocab_list:\n",
    "                    vocab_list.append(pos_tag)\n",
    "                \n",
    "                pos_tags.append(vocab_list.index(pos_tag))\n",
    "    \n",
    "    if not vocab:\n",
    "        vocab_list.append('unk')\n",
    "        vocab_list = {vocab_list[i]:i for i in range(len(vocab_list))}\n",
    "    \n",
    "    return Tensor(pos_tags), vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that combines the previous functions, and creates 2 tensors for a .conllu file: \n",
    "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
    "\n",
    "def create_data(filename: str, model, tokenizer, vocab=None):\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    \n",
    "    sen_reps = fetch_sen_reps(ud_parses, model, tokenizer)\n",
    "    pos_tags, pos_vocab = fetch_pos_tags(ud_parses, vocab=vocab)\n",
    "    \n",
    "    return sen_reps, pos_tags, pos_vocab\n",
    "\n",
    "\n",
    "use_sample = True\n",
    "\n",
    "train_x, train_y, train_vocab = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "    model, \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "dev_x, dev_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "    model, \n",
    "    tokenizer,\n",
    "    vocab=train_vocab\n",
    ")\n",
    "\n",
    "test_x, test_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "    model,\n",
    "    tokenizer,\n",
    "    vocab=train_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Classification\n",
    "\n",
    "We now have our models, our data, _and_ our representations all set! Hurray, well done. We can finally move onto the cool stuff, i.e. training the diagnostic classifiers (DCs).\n",
    "\n",
    "DCs are purposefully simple in their complexity. To read more about why this is the case you could already have a look at the \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2).\n",
    "\n",
    "A simple linear classifier will suffice for now, don't bother with adding fancy non-linearities to it.\n",
    "\n",
    "I am personally a fan of the `skorch` library, that provides `sklearn`-like functionalities for training `torch` models, but you are free to train your dc using whatever method you prefer.\n",
    "\n",
    "As this is an Artificial Intelligence master and you have all done ML1 + DL, I expect you to use your train/dev/test splits correctly ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
