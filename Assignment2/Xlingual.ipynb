{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device=device)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from conllu import parse_incr, TokenList\n",
    "from polyglot.mapping import Embedding\n",
    "from polyglot.downloader import downloader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nl_en = {}\n",
    "\n",
    "embeddings = Embedding.load(\"embeddings2/nl/embeddings_pkl.tar.bz2\")\n",
    "\n",
    "with open('data/filtered_en_nl_dict.txt', 'r') as bidict:\n",
    "    for trans in bidict:\n",
    "        nl, en = trans.split()\n",
    "        nl_en[nl] = en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(path):\n",
    "    proc_data = []\n",
    "    with open(path, 'r') as data: \n",
    "        for sentence in data:\n",
    "            proc_sentence = word_tokenize(sentence.lower())\n",
    "            proc_data.append(proc_sentence)\n",
    "    \n",
    "    return proc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess_data('data/train.txt')\n",
    "valid_data = preprocess_data('data/valid.txt')\n",
    "test_data = preprocess_data('data/test.txt')\n",
    "print(train_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "def fetch_sen_reps(data, model, tokenizer, embeddings, nl_en, model_type, device, concat=False):\n",
    "    \n",
    "    sen_reps_source = []\n",
    "    sen_reps_target = []\n",
    "    sen_len = []\n",
    "    for sentence in tqdm(data):\n",
    "        output_source = []\n",
    "        if model_type=='TF':\n",
    "            total_tokens = [101]\n",
    "\n",
    "            for word in sentence:\n",
    "                if word in embeddings.vocabulary:\n",
    "                    nl_embed = Tensor(embeddings.get(word)).to(device)\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if word in nl_en:\n",
    "                    en_word = nl_en[word]\n",
    "                    input_ids = tokenizer.encode(en_word)[1]\n",
    "                elif word in string.punctuation:\n",
    "                    input_ids = tokenizer.encode(word)[1]\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                output_source.append(nl_embed)\n",
    "                total_tokens.append(input_ids)\n",
    "\n",
    "            total_tokens.append(102)\n",
    "            output_source = torch.stack(output_source)\n",
    "\n",
    "            input_sen = Tensor(total_tokens).type(torch.long).unsqueeze(0).to(device)\n",
    "            output_sen = model(input_sen)[0][0][1:-1].detach()\n",
    "\n",
    "            if concat:\n",
    "                sen_reps_target.extend(output_sen.cpu())\n",
    "                sen_reps_source.extend(output_source.cpu())\n",
    "                \n",
    "\n",
    "            else:\n",
    "                sen_reps_target.append(output_sen.cpu())\n",
    "                sen_reps_source.append(output_source.cpu())\n",
    "\n",
    "            sen_len.append(output_sen.size(0))\n",
    "\n",
    "    if concat:\n",
    "        return torch.stack(sen_reps_target), torch.stack(sen_reps_source)\n",
    "\n",
    "    else:\n",
    "        return sen_reps_target, sen_reps_source, Tensor(sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_reps_target, sen_reps_source, sen_len = fetch_sen_reps(train_data[0:2000], model, tokenizer, embeddings, nl_en, 'TF', device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
